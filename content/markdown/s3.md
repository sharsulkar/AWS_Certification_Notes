# Simple Storage Service (S3)  
**Service category:** Object storage  
**Resiliency scope:** Region  
**Service scope:** Global  
**Service access:** Public  
**Service model:**  
**key words:**   
---
 
## Internal Architecture  
### Buckets  
* container for objects  
* per account limits - 100 soft limit, 1000 hard limit  
* created in a specific region - primary home region. Data does not leave this region unless explicitly specified by design  
* Bucket naming conventions -  
  1. bucket name has to be **globally unique**, no one else can use the name in any other AWS account in any region  
  2. 3-63 characters, all lower case, no underscores  
  3. can start with letter or number  
  4. cant be formatted as IP addresses eg 1.1.1.1  
* they can hold unlimited number of objects  
* it has a flat structure, everything is at root level - no hierarchical folder structure. The files can be arranged into folder-like structure by using **prefixes**  

### Objects  
* files stored as key-value pairs
* object size can range from **0 to 5TB**  
* Object structure -  
 1. object key - unique identifier, can be used to refer the object along with bucketname  
 2. value - object data  
 3. Version ID  
 4. Metadata  
 5. Access control  
 6. Subresources  

#### Object versioning
* Object versioning is a feature which can be enabled on an S3 bucket - allowing the bucket to store multi versions of objects  
* 
* When new bucket is created, versioning is disabled. It can be enabled on the bucket after creation. The versioning can be suspended on an enabled bucket. 
**but once enabled, versioning cannot be disabled on the bucket** 
* Without versioning, any object can be uniquely identified in a bucket with its key. Any changes overwrites the object.  
* With versioning enabled, the object key+versionid+bucketname is the unique identifier. If versionid is not mentioned, the latest version will always be used.  
* All versions are immutable, any changes creates a new version of the object. The old version is unchanged.  
* **Delete Marker** - If object is manually deleted without providing a specific versionid, S3 creates a special version called Delete marker as the latest version 
and hides the object from view. The object is not actually deleted from S3. This delete marker can be deleted as well reinstating the object.  
* If object is manually deleted by providing a version id, that version is deleted from S3. If latest version is deleted in this way, the most recent version is tagged as latest.  
* Each version counts towards storage costs and is billed.  
* **MFA Delete** - setting in version control, if enabled, Multifactor authentication is required to change bucket versioning state or delete versions.  

## Security and Access control  
### S3 bucket policy  
* It is a type of AWS resource policy (it provides resource perspective to permissions)  
* It can provide access to identities from same or different accounts  
* It can provide access to anonymous principals - can be used for static hosting of websites  
* Similar json structure as identity policy, in addition, there is an explicit **"Principal" section**  
* examples - https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html  

### Access Control List (ACL)  
* legacy - not recommended to use  
* Object subresource  
* inflexible compared to bucket policy  

### Block Public Access settings  
* will only apply anonymous principals that do not have AWS identities  
* as a fail safe to avoid unintentional public access to S3 due to wrongly written permission and bucket policies  
* they can be configured during bucket creation or afterwards and have multiple, flexible options to choose from  

## S3 performance optimization 
* By default, S3 uploads (PutObject action) occer over a single data stream. if stream fails, upload fails and entire upload needs to restart. This is not optimal for large uploads.  
* **Multipart upload** solves this. Data is broken up. Min size for data for multipart upload is 100MB. Upload can be split into max 10,000 parts, each with size 5MB - 5GB. Last part can be smaller than 5MB. Parts can fail and can be restarted without impacting upload of other parts.  
* **S3 Transfer Acceleration** - Using public internet network and routing is not optimal for data transfers because data routing decision and path taken by data depends on data traffic loads and what is commercially viable for them. Public internet is designed for resiliancy and flexibility. Fastest possible data transfer is not their goal. To ensure best performance, S3 can use regional egde locations and AWS global network to bypass the public internet for inter-regional data transfers. Data transfer from client to edge location might happen on public internet but it is fast as the edge location closest to the client is chosen. There are some restrictions to use.  
  1. Transfer acceleration is disabled by default.  
  2. The bucket names cannot have periods, names should be DNS compatible. 

## Service Billing  
https://aws.amazon.com/s3/pricing/  

## Encryption  
* buckets are not encrypted. Objects inside the bucket are.  
* Bucket default encryption - the default encryption to be used if object encryption is not explicitly specified. If object level encyption is explicitly specified, it overrides the default.  
* encryption in transit comes default with s3  
### Encryption at rest  
* client side - happens on client side before it moves to S3, all encryption and key management is handled by client  
* server side encryption -   
* 2 operations needed for server side encryption - managing keys and encrypting/decrypting data  
* 3 types available based on how the above two points are handled  
 1. **Serverside encryption with customer provided keys (SSE-C)**  
 * Customer manages keys, S3 manages encryption/decryption of data  
 * admin overhead of keys management  
 * for encryption operation - client provides data and encryption key to S3, S3 encrypts data using key and stores the hash of the key with data and the original key is discarded.  
 * for decryption operation - client provides encrypted file and key used for encryption to S3. Key is used to compare with the hash stored during encryption as a form of  authentication. The key is used for decrypting the file and is then discarded. S3 returns the decrypted file.  
 * use cases - regulation heavy environments  
 
 2. **Serverside encryption with S3 managed keys (SSE-S3)**  
 * Uses AES-256 encryption algorithm (symmetric encryption)  
 * S3 manages keys as well as encryption.   
 * 2 types of keys are involved -
   a. Master key - single key generated and managed by S3.  
   b. Data key - separate key unique to each object.  
 * encryption operation - client provides plaintext data. S3 generates a unique plaintext data key for that object. The plaintext data is encrypted using the data key. The master key is used to encrypt the data key. The plain text data key is discarded. The encrypted data and data key are stored together.  
 * decryption operation - client requests a decryption operation mentioning the object name. S3 uses the master key to decrypt the data key. The plaintext data key is then used to decrpyt the ciphertext data. The plaintext data key is discarded. Plaintext data is passed on to client.  
 * Limitations -  
  1. Role separation not possible. S3 admin can decrypt data and there is no way to limit that.  
  2. Key management is not possible, might be needed for regulated industries.  

 3. **Serverside encryption with CMKs stored in KMS (SSE-KMS)**  
 * KMS service used for key management. S3 does the encryption/decryption operation.  
 * Very similar to SSE-S3 with below differences  
  1. Instead of Master key, SSE-KMS uses CMK that is generated and managed by KMS. The CMK can be AWS managed or customer managed. With customer managed CMK, customer has full control over managing the CMK.  
  2. Unique data keys are generated by KMS for each S3 object.  
 * Benefits over SSE-S3 -  
  1. Key management if customer managed CMK is used  
  2. Role separation - S3 admins do not have access to CMK unless explicitly given. so different groups can be perform encryption, decryption and key management without permission overlap  

https://docs.aws.amazon.com/AmazonS3/latest/user-guide/default-bucket-encryption.html  
https://docs.aws.amazon.com/kms/latest/developerguide/services-s3.html  
https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html  
https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html  
https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerSideEncryptionCustomerKeys.html  

## Storage classes  
https://aws.amazon.com/s3/storage-classes/  
**storage class is at object level**  

### S3 standard  
* default storage class  
* **best used for - frequently accessed, important, non-replacable data**  
* durability - 11 9's  
* data replication - atleast 3 AZs within a region, MD5 and CRC used to detect and correct any replication errors  
* when object has been stored durably (replication and error check conditions met), S3 responds with HTTP/1.1 200 OK  
* latency - milisecs, makes data available for use instantly  
* content can be made publicly available  
* Billing - 
 1. GB/month fee for data stored  
 2. $ per GB charge for transfer out (transfer in is free)  
 3. price per 1000 requests  
 4. No retreival fee  
 5. No minimum duration  
 6. No minimum size  

### S3 standard-IA  
* **best used for - long-lived, important, non-replacable data needing infrequent acces**  
* durability, latency, replication and public access to data same as S3 standard  
* Billing - 
 1. GB/month fee for data stored half of S3 Standard  
 2. $ per GB charge for transfer out (transfer in is free)  
 3. price per 1000 requests  
 4. retreival fee  
 5. minimum duration of 30 days 
 6. minimum size 128KB  

### S3 One Zone-IA  
* **best used for - long-lived, non-important, replacable data needing infrequent acces**  
* data stored in only one zone, replication happens within that AZ  
* Billing - same as S3 standard-IA with cheaper storage costs  

### S3 Glacier  
* **best used for -archival data, no realtime access, min-hours latency is acceptable**  
* to store cold data - data is not instantly available for use  
* durability, latency, replication same as S3 standard  
* no public access to data  
* each data retreival costs money - data is stored temporarily in S3 Standard-IA bucket  
* 3 types of retreival latency - faster=expensive  
 1. expediated - data available in 1-5 mins  
 2. standard - 3-5 hrs  
 3. bulk - 5-12 hrs  
* Billing - 
 1. GB/month fee for data stored 1/5th of S3 Standard  
 2. $ per GB charge for transfer out (transfer in is free)  
 3. price per 1000 requests  
 4. retreival fee based on latency mentioned above  
 5. minimum duration of 90 days 
 6. minimum size 40KB  

### S3 Glacier Deep archive  
* **best used for - archival data eg legal hold or regulatory requirement data, very rarely accessed, hours-days latency is acceptable**  
* to store frozen data - data is not instantly available for use  
* durability, latency, replication same as S3 standard  
* no public access to data  
* each data retreival costs money - data is stored temporarily in S3 Standard-IA bucket  
* 2 types of retreival latency - faster=expensive  
 1. standard - 12 hrs  
 2. bulk - upto 48 hrs  
* Billing - 
 1. GB/month fee for data stored 1/4th of S3 Glacier  
 2. $ per GB charge for transfer out (transfer in is free)  
 3. price per 1000 requests  
 4. retreival fee based on latency mentioned above  
 5. minimum duration of 180 days 
 6. minimum size 40KB  

### S3 Intelligent tiering  
* **best used for - long-lived data where usage is changing or unknown**  
* dynamic storage class - 4 different tiers similar to static storage classes mentioned above  
* Data can be configured to move automatically between tiers based on access patterns  
* billing structure is same as corresponding static classes, but there is an additional management fee for automation  
* Frequent Access = S3 standard  
* Infrequent Access = S3 standard IA  
* Archive = S3 Glacier  
* Deep Archive = S3 glacier deep archive  

## Lifecycle configuration  
* set of rules to manage lifecycle of objects in a S3 bucket  
* consist of actions based on some criteria  
* can apply to entire bucket or group of objects identified by prefixes or tags  
* **Action criteria cannot be based on access patterns. it is only supported by intelligent tiering**  
* 2 types of actions  
 1. Transition actions - changes storage class of objects 
 * objects can move downwards only - standard -> IA -> Archive. Upward movement requires retreival jobs that costs  
 * object in S3 standard have to stay there for 30 days before they can be moved using transition action  
 * if a single rule is used to move from standard to IA and then to archive, object has to stay in each class for atleast 30 days    
 2. Expiration actions - delete objects or their versions  

## S3 select and Glacier select  
* SQL like statements can be used to prefilter objects stored in S3 and Glacier  

## Replication  
* Cross region replication(CRR) - source and destination buckets in different region  
* Same region replication(SRR) - source and destination buckets in the same region  
* Replication configuration is applied to the source bucket  
* Configuration items  
 1. Destination bucket  
 2. IAM role with appropriate trust and permission policies  
 3. if source and destination accounts are separate, bucket policy needed on destination bucket that allows the role write access  
 4. what to replicate - default is all objects, subset can be selected  
 5. what storage class - default is to use the same as source bucket  
 6. ownership of destination bucket - default is the source account (even if destination bucket is in different account)  
 7. Replication Time control (RTC) - optional feature that adds a SLA that guarantees a 15 min replication, default is best effort replication timeframe  
 
* replication in not retroactive - it starts for object versions modified/created after replication is turned on. Object version that exist in source bucket before replication is turned on are not replicated.  
* replication needs versioning to be turned on  
* replication is a one-way process - objects are replicated from source to destination only  
* objects with SSE-C cannot be replicated, other SSE encryptions work fine  
* cannot replicate objects using glacier and glacier deep archive storage classes  
* cannot replicate actions performed by lifecycle management on source bucket objects  
* any deletes are not replicated  

* Why use replication -  
 1. SRR - log aggregation, prod test data sync, resilience with strict sovereignity  
 2. CRR - improve global resilience, latency reduction  

* important links  
https://docs.aws.amazon.com/AmazonS3/latest/dev/replication-what-is-isnot-replicated.html  
https://docs.aws.amazon.com/AmazonS3/latest/dev/replication.html  
https://aws.amazon.com/about-aws/whats-new/2019/11/amazon-s3-replication-time-control-for-predictable-replication-time-backed-by-sla/  

## Presigned URLs  
* used to access objects in a private bucket with the same access permissions as the IAM identity that generated the presigned url has at the time the url is used  
* the access is time controlled and the url expires after the specified time limit  
* you can create a presigned url to an object you dont have access to, the user of the url will also not have access as it is linked to you  
* always use long term identities like iam user to generate url. not a good practice to generate urls using roles as the url will stop working when the temporary credentials of the role expire, which is usually much earlier than the need to access the url   

## S3 events  
* notification generated when event occur in bucket (events include create, delete, restore, replicate)  
* can be delivered to SNS, SQS and lambda functions - for use with event driven architecture  
* events are configured using config file and event notifications are generated as json files  
* S3 events is older service, Event brigde offers a more functionality for event notification  

## Access logs  
* enable logging on the bucket is needed, logs are stored in a separate target bucket  
* logging managed by S3 log delivery group, it needs to be given write access to the destination bucket  
* single target bucket can be used for multiple source buckets and can be separated using prefixes  
* lifecycle of log files need to be managed by the user, its not managed by AWS automatically  

## S3 requestor pays  
* storage cost is paid for owner but transfer charges are paid by requestor  
* requestor has to be a AWS accounts  
* cannot use static hosting  

## S3 object lock  
* can be enabled on new bucket, for existing buckets - contact aws support  
* versioning must be turned on, cannot be turned off or suspended  
* object lock implements write-once-read-many (WORM) model on individual versions, they cannot be overwritten or deleted  
* 2 types of retention methods - object version can have one or both or none  
 1. Retention period - 
 * specify retention period in days and years  
 * compliance mode - until retention period expires, retention period settings cannot be adjusted, object cannot be modified in any way - not even by account root user  
 * governance mode - less strict version of compliance mode. special permissions can be granted to identities to allow changing the lock settings  
 2. legal hold  
 * no retention period, its binary - on or off  
 * can be used for accidental deletions of critical object versions or for legal hold requirements   

## Use cases  
### Static hosting  
* feature can be enabled using S3  
* AWS creates website endpoint using the bucketname and region. For custom domains, the domain name must match with the bucketname  
* Static website hosting like blogs etc  
* Offloading static content for dyanamic content of a website  
* out-of-band pages - if server is offline due to maintance or issues, any error pages can be hosted in a different region on S3.  
*                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   

## Related services  
### In the same category  
### With the same scope  
### Most commonly used with  
## AWS whitepapers  
